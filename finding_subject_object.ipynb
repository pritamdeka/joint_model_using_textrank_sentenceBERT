{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "finding_subject_object.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "B252RIJdKLTz",
        "colab_type": "code",
        "outputId": "bdd06204-28d8-4f1c-c56c-fc6ea89cae3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "########## This code is for finding out the subject object from the heading\n",
        "\n",
        "\n",
        "import spacy\n",
        "subject_tokens=[]\n",
        "object_tokens=[]\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "sentence_main=\"graviola shows promise in cancer cures and arthritis.\"\n",
        "doc = nlp(sentence_main)\n",
        "for chunk in doc.noun_chunks:\n",
        "    #print(chunk.text, chunk.root.text, chunk.root.dep_,chunk.root.head.text)\n",
        "    print(chunk.root.text, chunk.root.dep_)\n",
        "    if (chunk.root.dep_==\"nsubj\"):\n",
        "      subject_tokens.append(chunk.root.text)\n",
        "      print(subject_tokens)\n",
        "    elif (chunk.root.dep_==\"dobj\" or chunk.root.dep_==\"pobj\"):\n",
        "      object_tokens.append(chunk.text)\n",
        "      print(object_tokens)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "graviola nsubj\n",
            "['graviola']\n",
            "cures pobj\n",
            "['cancer cures']\n",
            "arthritis conj\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sz20ghk4awKO",
        "colab_type": "code",
        "outputId": "c0d73bfd-1f60-4308-e3c0-e43ceb0bdf86",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "####### This code is for finding those sentences which contain the subject and the object\n",
        "\n",
        "\n",
        "import re\n",
        "import pandas as pd\n",
        "line=[]\n",
        "can_rel=[]\n",
        "df=pd.read_excel(r'/content/pandas_simple.xlsx')\n",
        "df['Body'] = df['Body'].str.lower()\n",
        "for i in range(len(df.Body)):\n",
        "    #if(i<2000):\n",
        "    line.append(df.Body[i])\n",
        "#print(line)\n",
        "for sent in line:\n",
        "    match = re.findall(\".*graviola.*cancer.*\", sent)   ##### returns back sentences which has the word that followed by subject and object\n",
        "    #print(match)\n",
        "    for line in match:\n",
        "        #print(line)\n",
        "        can_rel.append(line)\n",
        "print(can_rel)\n",
        "        #print(re.findall('\\\".*\\\"', df['Body']))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['the first modern-day research on graviola was conducted in 1976 by the national cancer institute, though the plant has been under investigation since the 1940s.', 'their findings reported that the leaves of the graviola plant were effective in destroying malignant cancer cells.', 'a korean study found that graviola killed colon cancer cells better than a chemotherapy drug called adriamycin.', 'this means that there would likely be no hair loss or nausea as side effects from using graviola as a treatment for cancers.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S48ld83WsyQF",
        "colab_type": "code",
        "outputId": "48d0ac46-dbcc-462a-c28d-38597b2d8dc1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "\n",
        "####### This code is for finding the score of each of the sentence from previous part by applying TextRank\n",
        "\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        " \n",
        "from nltk.tokenize.punkt import PunktSentenceTokenizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
        "score_textrank=[] \n",
        "sentences=can_rel\n",
        "def textrank(can_rel):\n",
        "    \n",
        "    bow_matrix = CountVectorizer().fit_transform(sentences)\n",
        "    normalized = TfidfTransformer().fit_transform(bow_matrix)\n",
        " \n",
        "    similarity_graph = normalized * normalized.T\n",
        " \n",
        "    nx_graph = nx.from_scipy_sparse_matrix(similarity_graph)\n",
        "    scores = nx.pagerank(nx_graph)\n",
        "    for i in range(len(scores)):\n",
        "      #print(scores[i])\n",
        "      score_textrank.append(scores[i])\n",
        "    #print(((scores[i],s) for i,s in enumerate(sentences)))\n",
        "    x= list((scores[i],s) for i,s in enumerate(sentences))\n",
        "    print(x)\n",
        "    return (((scores[i],s) for i,s in enumerate(sentences)))\n",
        "                  #reverse=True)\n",
        "out=textrank(can_rel)\n",
        "print(out)\n",
        "score_textrank_sorted=sorted(score_textrank, reverse=True)\n",
        "print(score_textrank_sorted)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(0.2525267693088695, 'the first modern-day research on graviola was conducted in 1976 by the national cancer institute, though the plant has been under investigation since the 1940s.'), (0.27218812841871776, 'their findings reported that the leaves of the graviola plant were effective in destroying malignant cancer cells.'), (0.23893664841881274, 'a korean study found that graviola killed colon cancer cells better than a chemotherapy drug called adriamycin.'), (0.23634845385359998, 'this means that there would likely be no hair loss or nausea as side effects from using graviola as a treatment for cancers.')]\n",
            "<generator object textrank.<locals>.<genexpr> at 0x7f8732f895c8>\n",
            "[0.27218812841871776, 0.2525267693088695, 0.23893664841881274, 0.23634845385359998]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AoWI25RWdoZZ",
        "colab_type": "code",
        "outputId": "e3194be7-6589-4f00-a528-1288a2341539",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        }
      },
      "source": [
        "######## This code gives the cosine score of each sentence by using sentence BERT\n",
        "\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import scipy.spatial\n",
        "import gensim\n",
        "import string\n",
        "import sys\n",
        "#sys.stdout = open('C:/Users/Pritam/Desktop/Scores/sentence_sim_scores.txt', 'w', encoding=\"utf-8\")\n",
        "#from nltk.corpus import stopwords\n",
        "#stop_words = set(stopwords.words('english'))\n",
        "#sentence_list=[]\n",
        "embedder = SentenceTransformer('roberta-large-nli-stsb-mean-tokens')\n",
        "\n",
        "from nltk import sent_tokenize\n",
        "corpus = can_rel\n",
        "#print(sentences)\n",
        "print(can_rel)\n",
        "corpus_embeddings = embedder.encode(corpus)\n",
        "\n",
        "# Query sentences:\n",
        "from nltk import sent_tokenize\n",
        "queries = sent_tokenize(sentence_main)\n",
        "print(queries)\n",
        "\n",
        "query_embeddings = embedder.encode(queries)\n",
        "\n",
        "# Find the closest  sentences of the corpus for each query sentence based on cosine similarity\n",
        "closest_n = 10\n",
        "score_list=[]\n",
        "for query, query_embedding in zip(queries, query_embeddings):\n",
        "    distances = scipy.spatial.distance.cdist([query_embedding], corpus_embeddings, \"cosine\")[0]\n",
        "    print(distances)\n",
        "    print(range(len(distances)))\n",
        "    results = zip(range(len(distances)), distances)\n",
        "    x=list(results)\n",
        "    print(x)\n",
        "    #results = sorted(results, key=lambda x: x[1])\n",
        "    #print(results)\n",
        "\n",
        "    print(\"\\n\\n======================\\n\\n\")\n",
        "    print(\"Query:\", query)\n",
        "    print(\"\\nTopmost similar sentences in corpus:\")\n",
        "\n",
        "    for idx, distance in x[0:closest_n]:\n",
        "    \tscore=1-distance\n",
        "      \n",
        "    \tprint(corpus[idx].strip(), \"(Score: %.4f)\" % (score))\n",
        "    \tscore_list.append(score)\n",
        "print(score_list)\n",
        "#from statistics import mean\n",
        "#avg=mean(score_list)\n",
        "#print(\"The average of the scores is: %.4f\" % avg)\n",
        "#sys.stdout.close()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['the first modern-day research on graviola was conducted in 1976 by the national cancer institute, though the plant has been under investigation since the 1940s.', 'their findings reported that the leaves of the graviola plant were effective in destroying malignant cancer cells.', 'a korean study found that graviola killed colon cancer cells better than a chemotherapy drug called adriamycin.', 'this means that there would likely be no hair loss or nausea as side effects from using graviola as a treatment for cancers.']\n",
            "['graviola shows promise in cancer cures and arthritis.']\n",
            "[0.58907033 0.38560702 0.38906662 0.44528538]\n",
            "range(0, 4)\n",
            "[(0, 0.5890703260663622), (1, 0.38560702269207614), (2, 0.3890666247090807), (3, 0.4452853761445118)]\n",
            "\n",
            "\n",
            "======================\n",
            "\n",
            "\n",
            "Query: graviola shows promise in cancer cures and arthritis.\n",
            "\n",
            "Topmost similar sentences in corpus:\n",
            "the first modern-day research on graviola was conducted in 1976 by the national cancer institute, though the plant has been under investigation since the 1940s. (Score: 0.4109)\n",
            "their findings reported that the leaves of the graviola plant were effective in destroying malignant cancer cells. (Score: 0.6144)\n",
            "a korean study found that graviola killed colon cancer cells better than a chemotherapy drug called adriamycin. (Score: 0.6109)\n",
            "this means that there would likely be no hair loss or nausea as side effects from using graviola as a treatment for cancers. (Score: 0.5547)\n",
            "[0.4109296739336378, 0.6143929773079239, 0.6109333752909193, 0.5547146238554882]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sTt3vdHydDBc",
        "colab_type": "code",
        "outputId": "abec79e5-8eec-4e18-caf4-8711ca208e6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        }
      },
      "source": [
        "######### The final code adds both the scores to get the final score according to which the highest score is given the most preference\n",
        "import math\n",
        "sum_list=[]     ########## list to hold the final sum of scores from TextRank and cosine score\n",
        "for sc, lst in zip(score_textrank, score_list):  ########## taking the scores from both the lists \n",
        "  \n",
        "  sig=(1 / (1 + math.e ** -(sc+lst)))   ########### using a sigmoid function to normalize the data so that it's between 0 and 1 everytime\n",
        "  sum_list.append(sig)    ########## adding all the above results in the list\n",
        "#print(sum_list)\n",
        "#print(range(len(sum_list)))\n",
        "result_final=zip(range(len(sum_list)),sum_list)      ####### adding the indexes to be used later\n",
        "result_final=sorted(result_final, key=lambda x: x[1],reverse=True)   ########## sorting in descending order \n",
        "for query, query_embedding in zip(queries, query_embeddings):\n",
        "    \n",
        "\n",
        "    print(\"\\n\\n======================\\n\\n\")\n",
        "    print(\"Query:\", query)\n",
        "    print(\"\\nTopmost similar sentences in corpus:\")\n",
        "\n",
        "    for idx, sum_list in result_final[0:closest_n]:\n",
        "    \t\n",
        "      \n",
        "    \tprint(corpus[idx].strip(), \"(Score: %.4f)\" % (sum_list))    ########## the final list of sentences in descending order with the normalized scores\n",
        "    \t#score_list.append(score)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.6600364025318065, 0.7081841292560026, 0.7005398762700901, 0.6880595483895238]\n",
            "range(0, 4)\n",
            "\n",
            "\n",
            "======================\n",
            "\n",
            "\n",
            "Query: graviola shows promise in cancer cures and arthritis.\n",
            "\n",
            "Topmost similar sentences in corpus:\n",
            "their findings reported that the leaves of the graviola plant were effective in destroying malignant cancer cells. (Score: 0.7082)\n",
            "a korean study found that graviola killed colon cancer cells better than a chemotherapy drug called adriamycin. (Score: 0.7005)\n",
            "this means that there would likely be no hair loss or nausea as side effects from using graviola as a treatment for cancers. (Score: 0.6881)\n",
            "the first modern-day research on graviola was conducted in 1976 by the national cancer institute, though the plant has been under investigation since the 1940s. (Score: 0.6600)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_z6dF9FVmHu2",
        "colab_type": "code",
        "outputId": "cd59a7dd-81e1-43c0-df3d-b1879b8748e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-uejI6LJerSt",
        "colab_type": "code",
        "outputId": "8f21b06d-443c-4566-97ba-528822e0f1f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 768
        }
      },
      "source": [
        "!pip install -U sentence-transformers"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sentence-transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/07/32/e3d405806ea525fd74c2c79164c3f7bc0b0b9811f27990484c6d6874c76f/sentence-transformers-0.2.5.1.tar.gz (52kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 2.4MB/s \n",
            "\u001b[?25hCollecting transformers==2.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/10/aeefced99c8a59d828a92cc11d213e2743212d3641c87c82d61b035a7d5c/transformers-2.3.0-py3-none-any.whl (447kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 10.1MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (4.38.0)\n",
            "Requirement already satisfied, skipping upgrade: torch>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (1.4.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (1.18.2)\n",
            "Requirement already satisfied, skipping upgrade: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (0.22.2.post1)\n",
            "Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: nltk in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (3.2.5)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 49.9MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers==2.3.0->sentence-transformers) (1.12.27)\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from transformers==2.3.0->sentence-transformers) (2.21.0)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/b4/7a41d630547a4afd58143597d5a49e07bfd4c42914d8335b2a5657efc14b/sacremoses-0.0.38.tar.gz (860kB)\n",
            "\u001b[K     |████████████████████████████████| 870kB 54.5MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==2.3.0->sentence-transformers) (2019.12.20)\n",
            "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sentence-transformers) (0.14.1)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from nltk->sentence-transformers) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.3.0->sentence-transformers) (0.3.3)\n",
            "Requirement already satisfied, skipping upgrade: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.3.0->sentence-transformers) (0.9.5)\n",
            "Requirement already satisfied, skipping upgrade: botocore<1.16.0,>=1.15.27 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.3.0->sentence-transformers) (1.15.27)\n",
            "Requirement already satisfied, skipping upgrade: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.3.0->sentence-transformers) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.3.0->sentence-transformers) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.3.0->sentence-transformers) (2019.11.28)\n",
            "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.3.0->sentence-transformers) (2.8)\n",
            "Requirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.3.0->sentence-transformers) (7.1.1)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.27->boto3->transformers==2.3.0->sentence-transformers) (2.8.1)\n",
            "Requirement already satisfied, skipping upgrade: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.27->boto3->transformers==2.3.0->sentence-transformers) (0.15.2)\n",
            "Building wheels for collected packages: sentence-transformers, sacremoses\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-0.2.5.1-cp36-none-any.whl size=67076 sha256=438867e7f74e2b76c5bf9f2dfa0c8acab23b8063e43a61c072099f679a6b28a0\n",
            "  Stored in directory: /root/.cache/pip/wheels/22/ca/b4/7ca542b411730a8840f8e090df2ddacffa1c4dd9f209684c19\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.38-cp36-none-any.whl size=884628 sha256=1935f6eca498115ff404b8866b8633d7986c89bb4c6568cda23fdb39c23fb0a4\n",
            "  Stored in directory: /root/.cache/pip/wheels/6d/ec/1a/21b8912e35e02741306f35f66c785f3afe94de754a0eaf1422\n",
            "Successfully built sentence-transformers sacremoses\n",
            "Installing collected packages: sentencepiece, sacremoses, transformers, sentence-transformers\n",
            "Successfully installed sacremoses-0.0.38 sentence-transformers-0.2.5.1 sentencepiece-0.1.85 transformers-2.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}